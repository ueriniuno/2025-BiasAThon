import argparse, os, gc, pandas as pd
from joblib import Parallel, delayed
from data_loader import load_data
from model_runner import load_pipeline_model, predict_batch_answers_with_pipeline
from prompt_engineer import make_prompt
import argparse, pandas as pd, os
import torch, warnings
warnings.filterwarnings("ignore", category=UserWarning)
print(load_data.__module__)
import inspect
print(inspect.getfile(load_data))



# ---------------- Args ------------- #
parser = argparse.ArgumentParser()
parser.add_argument("--input_csv", default="test.csv")
parser.add_argument("--output_csv", default="submission.csv")
parser.add_argument("--batch_size", type=int, default=10)
parser.add_argument("--dyn_batch",  type=int, default=2)
parser.add_argument("--max_new_tokens", type=int, default=32)
parser.add_argument("--num_workers", type=int, default=2)
parser.add_argument("--sample_size", type=int, default=None)   # Noneì´ë©´ ì „ì²´ ì‚¬ìš©
parser.add_argument("--seed", type=int, default=42)
args = parser.parse_args()
# ----------------------------------- #
# main.py  (import ë°”ë¡œ ì•„ë˜)
SAVE_EVERY = 500                     # âœ… 500í–‰ë§ˆë‹¤ ì €ì¥
RESUME = True                    # ì¤‘ê°„ CSVê°€ ìˆìœ¼ë©´ ì´ì–´ì„œ

# # ë°ì´í„° ë¡œë”©
# df = load_data(args.input_csv, sample_size=args.sample_size, seed=args.seed)

# # í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ ìƒì„±
# df["prompt"] = df.apply(lambda row: make_prompt(row["context"], row["question"], row["choices"]), axis=1)

# ëª¨ë¸ ë¡œë“œ (pipeline ê¸°ë°˜)
pipe = load_pipeline_model()

# ------------------------ #
# DataFrameì˜ ì¼ë¶€ chunkë¥¼ ë°›ì•„ì„œ í•œ ë²ˆì— ë°°ì¹˜ ë‹¨ìœ„ë¡œ inference ìˆ˜í–‰
def process_chunk(chunk):
    """chunk(DataFrame) ë‹¨ìœ„ infer"""
    prompts = chunk.apply(
        lambda row: make_prompt(row["context"], row["question"], row["choices"]),
        axis=1
    ).tolist()

    choices = chunk["choices"].tolist()

    p, r, a = predict_batch_answers_with_pipeline(pipe, prompts, choices, batch_size=args.batch_size)

    chunk["raw_input"] = p
    chunk["raw_output"] = r
    chunk["answer"] = a

    # âœ… ë°”ë¡œ ì €ì¥ (flush í•¨ìˆ˜ ì¬ì‚¬ìš©)
    _flush([chunk], header_written=os.path.exists(args.output_csv))
    print(f"ğŸ’¾ Saved chunk of {len(chunk)} rows â†’ {args.output_csv}")

    return chunk

def run_inference(input_csv: str, output_csv: str, batch_size: int):
    df = load_data(input_csv, sample_size=args.sample_size, seed=args.seed)

    if RESUME and os.path.exists(output_csv):
        done_df = pd.read_csv(output_csv)
        df = df[~df["ID"].isin(done_df["ID"])]
        print(f"â©  Resume mode: {len(done_df)} rows already done")

    n = len(df)
    print(f"ğŸ”¸ Remaining samples: {n}")

    chunk_size = 100
    chunks = [df.iloc[i:i+chunk_size] for i in range(0, n, chunk_size)]

    # ìˆ˜ì •
    # buffered, total_written = [], 0
    # header_written = os.path.exists(output_csv)
    #
    # for chunk in chunks:
    #     res = process_chunk(chunk.copy())
    #     buffered.append(res)

    #     if sum(len(x) for x in buffered) >= SAVE_EVERY:
    #         _flush(buffered, header_written)
    #         total_written += sum(len(x) for x in buffered)
    #         buffered.clear()
    #         header_written = True
    #         print(f"ğŸ’¾  {total_written} rows saved â†’ {output_csv}")

    # if buffered:
    #     _flush(buffered, header_written)
    #     total_written += sum(len(x) for x in buffered)
    #     print(f"ğŸ’¾  {total_written} rows saved (final)")

    for i, chunk in enumerate(chunks):
        _ = process_chunk(chunk.copy())
        gc.collect()
        
# ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ë“¤ í•˜ë‚˜ë¡œ ëª¨ìŒ. sample_submissionê³¼ mergeí•˜ì—¬ ëˆ„ë½ê°’ ë³´ì™„ 
def _flush(dfs, header_written):
    out_df = pd.concat(dfs).sort_index()
    out_df = out_df.drop(columns=["context", "question", "choices"], errors="ignore")
    # Load sample_submission and merge
    sample_df = pd.read_csv("sample_submission.csv")
    merged_df = sample_df.copy()
    merged_df = merged_df.merge(out_df, on="ID", how="left", suffixes=("", "_new"))

    for col in ["answer", "raw_input", "raw_output"]:
        if f"{col}_new" in merged_df.columns:
            merged_df[col] = merged_df[f"{col}_new"].combine_first(merged_df[col])
            merged_df.drop(columns=[f"{col}_new"], inplace=True)

    out_df = merged_df
    mode   = "a" if header_written else "w"
    out_df.to_csv(args.output_csv, mode=mode, index=False, encoding="utf-8-sig",
                  header=not header_written)

if __name__ == "__main__":
    run_inference(args.input_csv, args.output_csv, args.batch_size)