# main.py
import argparse
import gc
import torch
from tqdm import tqdm 
import pandas as pd

from data_loader import load_data
from model_runner import load_model, predict_batch_answers

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ê¸°ë³¸ ì„¤ì •  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
torch.backends.cudnn.benchmark = True          # MPS/CPU ì†ë„ íŠœë‹
torch.manual_seed(42)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ì¶”ë¡  ë£¨í”„  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from typing import Optional

def run_inference(
    input_csv: str,
    output_csv: str,
    batch_size: int = 100,
    resume_from_id: Optional[str] = None,
    dyn_batch: int = 2,
    max_new_tokens: int = 32,
):
    # 1) ë°ì´í„° Â· ëª¨ë¸ ë¡œë“œ
    data = load_data(input_csv)
    tokenizer, model = load_model()

    # 2) ì—´ íƒ€ì… ê³ ì • (ê²½ê³  ë°©ì§€)
    for col in ["raw_input", "raw_output", "answer"]:
        data[col] = data[col].astype("string")

    # 3) ì¬ì‹œì‘ ìœ„ì¹˜ ê²°ì •
    if resume_from_id:
        idx_list = data.index[data["ID"] == resume_from_id].tolist()
        if not idx_list:
            raise ValueError(f"{resume_from_id} not found in the dataset.")
        start_idx = idx_list[0] + 1
        print(f"ğŸ”„  {resume_from_id} ì´í›„ index {start_idx}ë¶€í„° ì¬ì‹œì‘")
    else:
        start_idx = 0

    # 4) ë°°ì¹˜ ì¶”ë¡ 
    for i in tqdm(range(start_idx, len(data), batch_size), desc="Processing"):
        batch = data.iloc[i : i + batch_size]

        prompts, raws, answers = predict_batch_answers(
            tokenizer,
            model,
            batch["context"].tolist(),
            batch["question"].tolist(),
            batch["choices"].tolist(),
            max_new_tokens=max_new_tokens,
            dyn_bs=dyn_batch,
        )

        data.loc[batch.index, "raw_input"]  = prompts
        data.loc[batch.index, "raw_output"] = raws
        data.loc[batch.index, "answer"]     = answers

        # 5) ì¤‘ê°„ ì €ì¥
        if i % 500 == 0 and i > 0:
            path = f"submission_checkpoint_{i}.csv"
            tqdm.write(f"ğŸ’¾  {i}/{len(data)} ì €ì¥ â†’ {path}")
            data[["ID", "raw_input", "raw_output", "answer"]].to_csv(
                path, index=False, encoding="utf-8-sig"
            )
            torch.mps.empty_cache(); gc.collect()

    # 6) ìµœì¢… ì €ì¥
    data[["ID", "raw_input", "raw_output", "answer"]].to_csv(
        output_csv, index=False, encoding="utf-8-sig"
    )
    print(f"ğŸ‰  ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_csv}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  CLI  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--batch_size", type=int, default=100, help="CSV ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--dyn_batch",  type=int, default=2,   help="ëª¨ë¸ ì…ë ¥ ë™ì  ë°°ì¹˜(1â€‘2 ê¶Œì¥)")
    parser.add_argument("--resume_from_id", type=str, default=None, help="ì¬ì‹œì‘í•  ID")
    parser.add_argument("--max_new_tokens", type=int, default=32, help="generate í† í° ê¸¸ì´")
    args = parser.parse_args()

    run_inference(
        "Yeogyeong/test.csv",
        "Yeogyeong/baseline_submission.csv",
        batch_size=args.batch_size,
        resume_from_id=args.resume_from_id,
        dyn_batch=args.dyn_batch,
        max_new_tokens=args.max_new_tokens,
    )
