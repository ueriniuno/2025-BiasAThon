# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  model_runner.py  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, re, gc, time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from prompt_engineer import make_prompt
from postprocessor import extract_answer
from tqdm.auto import tqdm     # â† ì§„í–‰ë¥  ë°”

_RE_ABC = re.compile(r"^\s*([ABC])\b")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  1) ì „ì—­ ìºì‹œì— í•œ ë²ˆë§Œ ë¡œë“œ  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_MODEL_CACHE = None        # (tokenizer, model) íŠœí”Œ

def load_model(model_name="meta-llama/Llama-3.1-8B-Instruct"):
    """
    Llama-3 8Bë¥¼ **ìµœì´ˆ 1íšŒë§Œ** ë¡œë“œí•˜ê³  warm-up í•œë‹¤.
    ë‘ ë²ˆì§¸ í˜¸ì¶œë¶€í„°ëŠ” ì „ì—­ ìºì‹œë¥¼ ê·¸ëŒ€ë¡œ ëŒë ¤ì¤€ë‹¤.
    """
    global _MODEL_CACHE
    if _MODEL_CACHE is not None:        # ì´ë¯¸ ë¶ˆëŸ¬ì™”ìœ¼ë©´ ê·¸ëŒ€ë¡œ
        return _MODEL_CACHE

    token     = os.getenv("HF_TOKEN")
    tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)
    tokenizer.pad_token    = tokenizer.eos_token
    tokenizer.padding_side = "left"

    print("ğŸ”§  Llama 8B warm-up (1íšŒ)â€¦")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map={"": "mps"},         # Mac M-ì‹œë¦¬ì¦ˆ
        token=token,
        torch_dtype=torch.float16
    )
    if hasattr(torch, "compile"):
        try:
            model = torch.compile(model)
        except Exception:
            pass

    # warm-up
    dummy = tokenizer("Warm-up", return_tensors="pt").to("mps")
    with torch.no_grad():
        model.generate(**dummy, max_new_tokens=1)
    print("âœ…  warm-up done\n")

    _MODEL_CACHE = (tokenizer, model)
    return _MODEL_CACHE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _tokenize(tok, texts):
    return tok(
        texts, return_tensors="pt",
        truncation=True, max_length=512, padding=True
    ).to("mps")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  2) ë°°ì¹˜-ë‹¨ìœ„ ì¶”ë¡  + ë¡œê·¸  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def predict_batch_answers(
    tokenizer, model,
    contexts, questions, choices_list,
    max_new_tokens: int = 32,
    dyn_bs: int = 2,
):
    """
    dyn_bs(=2)ì”© ì˜ë¼ì„œ generate â†’ ë‹¤ì‹œ í•©ì¹˜ê¸°
    * ğŸš€/âœ… ë¡œê·¸ë¡œ generate ì‹œì‘/ì¢…ë£Œ & ì†Œìš”ì‹œê°„ í™•ì¸
    * tqdm ì§„í–‰ë¥  ë°”(ì „ì²´ n ê°œ)
    """
    prompts, raws, answers = [], [], []
    n = len(contexts)

    # tqdm ì§„í–‰ë¥ 
    pbar = tqdm(total=n, unit="row", bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]")

    idx = 0
    while idx < n:
        bs = min(dyn_bs, n - idx)

        batch_prompts = [
            make_prompt(contexts[j], questions[j], choices_list[j])
            for j in range(idx, idx + bs)
        ]
        inputs = _tokenize(tokenizer, batch_prompts)

        # ----------------- generate -----------------
        t0 = time.perf_counter()
        print(f"ğŸš€  generate start: rows {idx}~{idx+bs-1}", flush=True)

        with torch.no_grad():
            out = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id,
            )

        dt = time.perf_counter() - t0
        print(f"âœ…  generate done : rows {idx}~{idx+bs-1}  ({dt:0.1f}s)", flush=True)
        # ------------------------------------------------

        decoded = tokenizer.batch_decode(out, skip_special_tokens=True)

        for k, res in enumerate(decoded):
            raw, ans = extract_answer(res, choices_list[idx + k])
            prompts.append(batch_prompts[k])
            raws.append(raw)
            answers.append(ans)

        idx += bs
        pbar.update(bs)

        # 100 rowë§ˆë‹¤ GPU ë©”ëª¨ë¦¬ ë¹„ìš°ê¸°
        if idx % 100 == 0:
            torch.mps.empty_cache()
            gc.collect()

    pbar.close()
    return prompts, raws, answers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€