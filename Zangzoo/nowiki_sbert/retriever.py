#retriever.py

import os,re, requests
import pickle
import faiss
import threading
from sentence_transformers import SentenceTransformer,CrossEncoder
from rank_bm25 import BM25Okapi
try:
    from konlpy.tag import Okt
    okt = Okt()
except:
    okt = None
import numpy as np
from sentence_transformers.util import cos_sim
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# SBERT & DB ì„¤ì •
_MODEL_NAME  = "jhgan/ko-sbert-nli"
_DB_TXT      = "bias_db.txt"
_IDX_FILE    = "faiss_bias.index"
_SENT_FILE   = "bias_sent.pkl"

# ì•™ìƒë¸”ìš© Cross-Encoder ëª¨ë¸ë“¤ (ì˜ì–´+í•œêµ­ì–´ íŠ¹í™”)
_CE_MODELS = [
    "cross-encoder/ms-marco-MiniLM-L-6-v2",
    "cross-encoder/stsb-distilroberta-base",
    "monologg/koelectra-base-v3-discriminator"
]
_CE_ENSEMBLE = [CrossEncoder(m) for m in _CE_MODELS]

# --- BM25 ì´ˆê¸°í™” ---
_BM25 = None
_BM25_SENTS = None
def _init_bm25(db_txt: str = _DB_TXT):
    global _BM25, _BM25_SENTS
    if _BM25 is None:
        sents = [ln.strip() for ln in open(db_txt, encoding="utf-8") if ln.strip()]
        if okt:
            tokenized = [okt.morphs(sent) for sent in sents]
        else:
            tokenized = [sent.split() for sent in sents]
        _BM25 = BM25Okapi(tokenized)
        _BM25_SENTS = sents
    return _BM25, _BM25_SENTS

# --- SBERT ëª¨ë¸ ë¡œë” (singleton) ---
_SBERT_MODEL = None
_SBERT_LOCK  = threading.Lock()
def _get_sbert():
    global _SBERT_MODEL
    if _SBERT_MODEL is None:
        with _SBERT_LOCK:
            if _SBERT_MODEL is None:
                print("ğŸ”§  SBERT ë¡œë”©â€¦")
                _SBERT_MODEL = SentenceTransformer(_MODEL_NAME, device="mps")
                _SBERT_MODEL.encode(["warm-up"], convert_to_tensor=True)
                print("âœ…  SBERT ë¡œë”© ì™„ë£Œ")
    return _SBERT_MODEL

# --- Cross-Encoder ì¬ë­í‚¹ (ì•™ìƒë¸”) ---
def rerank_with_cross_encoder(query: str, docs: list[str]) -> list[str]:
    """
    Cross-Encoder ì•™ìƒë¸”ë¡œ í›„ë³´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¬ë­í‚¹í•©ë‹ˆë‹¤.
    ê° ëª¨ë¸ë¡œ ì˜ˆì¸¡í•œ ì ìˆ˜ë¥¼ í‰ê·  ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
    """
    pairs = [[query, d] for d in docs]
    all_scores = [ce.predict(pairs) for ce in _CE_ENSEMBLE]
    avg_scores = np.mean(all_scores, axis=0)
    # ì ìˆ˜ ìˆœì„œëŒ€ë¡œ ë„íë¨¼íŠ¸ ì •ë ¬
    ranked     = [doc for _, doc in sorted(zip(avg_scores, docs), key=lambda x: x[0], reverse=True)]
    return ranked

# --- MMR (Maximal Marginal Relevance) ---
def mmr(query: str, doc_sents: list[str], k: int, lambda_param: float) -> list[str]:
    sbert = _get_sbert()
    q_emb = sbert.encode([query], convert_to_tensor=True)
    d_embs = sbert.encode(doc_sents, convert_to_tensor=True)
    selected, selected_idx, candidates = [], [], list(range(len(doc_sents)))
    rel_scores = cos_sim(q_emb, d_embs)[0].cpu().tolist()
    for _ in range(min(k, len(candidates))):
        if not selected:
            idx = int(np.argmax(rel_scores))
        else:
            mmr_scores = []
            for i in candidates:
                diversity = max(cos_sim(d_embs[i].unsqueeze(0), d_embs[selected_idx]).cpu().tolist()[0])
                score = lambda_param * rel_scores[i] - (1 - lambda_param) * diversity
                mmr_scores.append((i, score))
            idx = max(mmr_scores, key=lambda x: x[1])[0]
        selected.append(doc_sents[idx])
        selected_idx.append(idx)
        candidates.remove(idx)
    return selected

# --- FAISS ì¸ë±ìŠ¤ ë¹Œë“œ ---
def build_index(db_txt: str = _DB_TXT):
    sents = [ln.strip() for ln in open(db_txt, encoding="utf-8") if ln.strip()]
    embs  = _get_sbert().encode(
        sents, convert_to_tensor=True, normalize_embeddings=True
    ).cpu().numpy()
    index = faiss.IndexFlatIP(embs.shape[1])
    index.add(embs)
    faiss.write_index(index, _IDX_FILE)
    pickle.dump(sents, open(_SENT_FILE, "wb"))
    print(f"âœ…  Bias DB indexed: {len(sents)} ë¬¸ì¥")

# --- í†µí•© ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤ ---
def get_relevant(query: str, k: int=5, method: str="all", pre_k: int=50, mmr_lambda: float=0.9) -> list[str]:
    """
    method:
      - 'bm25': BM25 ìƒìœ„ k
      - 'sbert': SBERT(FAISS) ìƒìœ„ k
      - 'all':   BM25 + SBERT ì¡°í•©
    """
    candidates = []
    # BM25
    if method in ("bm25", "all"):
        bm25, sents = _init_bm25()
        tokens      = okt.morphs(query) if okt else query.split()
        scores      = bm25.get_scores(tokens)
        top_idxs    = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]
        candidates.extend([sents[i] for i in top_idxs])
    # SBERT(FAISS)
    if method in ("sbert", "all"):
        if not os.path.exists(_IDX_FILE):
            build_index()
        index  = faiss.read_index(_IDX_FILE)
        sents  = pickle.load(open(_SENT_FILE, "rb"))
        qvec   = _get_sbert().encode([query], convert_to_tensor=True, normalize_embeddings=True).cpu().numpy()
        _, idxs= index.search(qvec, k)
        candidates.extend([sents[i] for i in idxs[0]])
    # ì¤‘ë³µ ì œê±°
    return list(dict.fromkeys(candidates))